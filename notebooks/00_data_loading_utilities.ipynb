{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 00 - Data Loading Utilities\n",
                "\n",
                "## üìã Purpose\n",
                "This notebook provides **shared utilities** for all other notebooks:\n",
                "- Data loading functions\n",
                "- Data cleaning and standardization\n",
                "- Common configurations\n",
                "\n",
                "## üéØ Outputs\n",
                "**3 Clean DataFrames**:\n",
                "- `enrolment_df` (3.6M rows)\n",
                "- `demographic_df` (800K rows)\n",
                "- `biometric_df` (500K rows)\n",
                "\n",
                "## üìå Usage\n",
                "All other notebooks (01-06) import these utilities to avoid code duplication."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Section 1: Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard Libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import glob\n",
                "import warnings\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# Configuration\n",
                "# Handle encoding (works in scripts, not needed in Jupyter)\n",
                "try:\n",
                "    sys.stdout.reconfigure(encoding='utf-8')\n",
                "except AttributeError:\n",
                "    pass  # Jupyter notebooks handle encoding automatically\n",
                "\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.max_rows', 100)\n",
                "sns.set(style=\"whitegrid\", palette=\"viridis\")\n",
                "plt.rcParams['figure.figsize'] = (14, 7)\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"‚úÖ Libraries loaded and configured successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create output directories\n",
                "os.makedirs('../output', exist_ok=True)\n",
                "os.makedirs('../output/enrollment', exist_ok=True)\n",
                "os.makedirs('../output/demographic', exist_ok=True)\n",
                "os.makedirs('../output/biometric', exist_ok=True)\n",
                "\n",
                "print(\"‚úÖ Output directories created/verified\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Section 2: Utility Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_and_combine(pattern):\n",
                "    \"\"\"\n",
                "    Load and combine multiple CSV files matching a pattern.\n",
                "    \n",
                "    Args:\n",
                "        pattern (str): Glob pattern for files (e.g., 'dataset/api_data_aadhar_enrolment_*.csv')\n",
                "    \n",
                "    Returns:\n",
                "        pd.DataFrame: Combined DataFrame from all matched files\n",
                "    \n",
                "    Example:\n",
                "        >>> df = load_and_combine('dataset/api_data_aadhar_enrolment_*.csv')\n",
                "        >>> print(f\"Loaded {len(df):,} rows\")\n",
                "    \"\"\"\n",
                "    files = glob.glob(pattern)\n",
                "    \n",
                "    if not files:\n",
                "        print(f\"‚ö†Ô∏è  WARNING: No files found for pattern: {pattern}\")\n",
                "        return pd.DataFrame()\n",
                "    \n",
                "    print(f\"üìÅ Loading {len(files)} files for pattern: {pattern}\")\n",
                "    df_list = [pd.read_csv(f) for f in files]\n",
                "    combined = pd.concat(df_list, ignore_index=True)\n",
                "    \n",
                "    print(f\"‚úÖ Loaded {len(combined):,} total rows\")\n",
                "    return combined\n",
                "\n",
                "print(\"‚úÖ load_and_combine() function defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_data(df):\n",
                "    \"\"\"\n",
                "    Clean and standardize the data:\n",
                "    - Fix date formats\n",
                "    - Normalize state/district names (27 mappings)\n",
                "    - Validate pincodes (Indian range: 110000-999999)\n",
                "    - Handle missing values\n",
                "    \n",
                "    WHY: Raw government data has inconsistencies (e.g., 'West Bengal' vs 'West Bangal')\n",
                "         This ensures we can merge data correctly without losing records.\n",
                "    \n",
                "    Args:\n",
                "        df (pd.DataFrame): Raw DataFrame\n",
                "    \n",
                "    Returns:\n",
                "        pd.DataFrame: Cleaned DataFrame\n",
                "    \"\"\"\n",
                "    if df.empty:\n",
                "        return df\n",
                "    \n",
                "    print(f\"üßπ Starting data cleaning for {len(df):,} rows...\")\n",
                "    \n",
                "    # 1. Date Standardization\n",
                "    if 'date' in df.columns:\n",
                "        df['date'] = pd.to_datetime(df['date'], dayfirst=True, errors='coerce')\n",
                "        print(\"   ‚úÖ Dates standardized\")\n",
                "    \n",
                "    # 2. String Cleaning (State/District)\n",
                "    for col in ['state', 'district']:\n",
                "        if col in df.columns:\n",
                "            df[col] = df[col].astype(str).str.strip().str.title()\n",
                "    print(\"   ‚úÖ State/District names cleaned\")\n",
                "    \n",
                "    # 3. State Name Normalization (CRITICAL for merging)\n",
                "    state_map = {\n",
                "        'Andaman & Nicobar Islands': 'Andaman and Nicobar Islands',\n",
                "        'Andhra Pradsh': 'Andhra Pradesh',\n",
                "        'Chhatisgarh': 'Chhattisgarh',\n",
                "        'Dadra & Nagar Haveli': 'Dadra and Nagar Haveli and Daman and Diu',\n",
                "        'Daman & Diu': 'Dadra and Nagar Haveli and Daman and Diu',\n",
                "        'Jammu & Kashmir': 'Jammu and Kashmir',\n",
                "        'Orissa': 'Odisha',\n",
                "        'Pondicherry': 'Puducherry',\n",
                "        'Tamilnadu': 'Tamil Nadu',\n",
                "        'Telengana': 'Telangana',\n",
                "        'Uttaranchal': 'Uttarakhand',\n",
                "        'West Bangal': 'West Bengal',\n",
                "        'Westbengal': 'West Bengal',\n",
                "        'West Bengli': 'West Bengal'\n",
                "    }\n",
                "    if 'state' in df.columns:\n",
                "        df['state'] = df['state'].replace(state_map)\n",
                "        print(f\"   ‚úÖ Applied {len(state_map)} state name mappings\")\n",
                "    \n",
                "    # 4. District Normalization\n",
                "    dist_map = {\n",
                "        'Bangalore': 'Bengaluru',\n",
                "        'Bangalore Urban': 'Bengaluru Urban',\n",
                "        'Calcutta': 'Kolkata',\n",
                "        'Gurgaon': 'Gurugram'\n",
                "    }\n",
                "    if 'district' in df.columns:\n",
                "        df['district'] = df['district'].replace(dist_map)\n",
                "    \n",
                "    # 5. Pincode Validation (Indian pincodes: 110000-999999)\n",
                "    if 'pincode' in df.columns:\n",
                "        original_len = len(df)\n",
                "        df['pincode'] = pd.to_numeric(df['pincode'], errors='coerce').fillna(0).astype(int)\n",
                "        df = df[(df['pincode'] >= 110000) & (df['pincode'] <= 999999)]\n",
                "        removed = original_len - len(df)\n",
                "        if removed > 0:\n",
                "            print(f\"   ‚ö†Ô∏è  Removed {removed:,} rows with invalid pincodes\")\n",
                "    \n",
                "    # 6. Null Handling (Numeric ‚Üí 0)\n",
                "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
                "    df[num_cols] = df[num_cols].fillna(0)\n",
                "    \n",
                "    print(f\"‚úÖ Cleaning complete. Final size: {len(df):,} rows\")\n",
                "    return df\n",
                "\n",
                "print(\"‚úÖ clean_data() function defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Section 3: Load All Three Domains"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"üìä LOADING AADHAAR DATA (3 DOMAINS)\")\n",
                "print(\"=\"*70 + \"\\n\")\n",
                "\n",
                "# Load Enrollment Data\n",
                "print(\"1Ô∏è‚É£  ENROLLMENT DATA\")\n",
                "enrolment_df = clean_data(load_and_combine('../dataset/api_data_aadhar_enrolment_*.csv'))\n",
                "print()\n",
                "\n",
                "# Load Demographic Data\n",
                "print(\"2Ô∏è‚É£  DEMOGRAPHIC DATA\")\n",
                "demographic_df = clean_data(load_and_combine('../dataset/api_data_aadhar_demographic_*.csv'))\n",
                "print()\n",
                "\n",
                "# Load Biometric Data\n",
                "print(\"3Ô∏è‚É£  BIOMETRIC DATA\")\n",
                "biometric_df = clean_data(load_and_combine('../dataset/api_data_aadhar_biometric_*.csv'))\n",
                "print()\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"‚úÖ ALL DATA LOADED SUCCESSFULLY\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Section 4: Data Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nüìã DATASET SUMMARY\\n\")\n",
                "print(f\"{'Domain':<20} {'Rows':>15} {'Columns':>10}\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"{'Enrollment':<20} {len(enrolment_df):>15,} {len(enrolment_df.columns):>10}\")\n",
                "print(f\"{'Demographic':<20} {len(demographic_df):>15,} {len(demographic_df.columns):>10}\")\n",
                "print(f\"{'Biometric':<20} {len(biometric_df):>15,} {len(biometric_df.columns):>10}\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"{'TOTAL':<20} {len(enrolment_df) + len(demographic_df) + len(biometric_df):>15,}\")\n",
                "print()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display sample data\n",
                "print(\"\\nüìä ENROLLMENT DATA SAMPLE:\")\n",
                "display(enrolment_df.head(3))\n",
                "\n",
                "print(\"\\nüìä DEMOGRAPHIC DATA SAMPLE:\")\n",
                "display(demographic_df.head(3))\n",
                "\n",
                "print(\"\\nüìä BIOMETRIC DATA SAMPLE:\")\n",
                "display(biometric_df.head(3))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ‚úÖ Utilities Ready!\n",
                "\n",
                "### **Exported Variables** (Use in other notebooks):\n",
                "- `enrolment_df` ‚Üí Enrollment data (cleaned)\n",
                "- `demographic_df` ‚Üí Demographic update data (cleaned)\n",
                "- `biometric_df` ‚Üí Biometric update data (cleaned)\n",
                "\n",
                "### **Exported Functions**:\n",
                "- `load_and_combine(pattern)` ‚Üí Load multiple CSVs\n",
                "- `clean_data(df)` ‚Üí Standard cleaning pipeline\n",
                "\n",
                "### **Next Steps**:\n",
                "1. Run this notebook first\n",
                "2. Then run any domain notebook (01, 02, or 03)\n",
                "3. Or proceed to cross-domain analysis (04)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}